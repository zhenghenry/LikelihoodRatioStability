{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964af9f7-5757-4337-9885-9a372e67ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility imports\n",
    "# General imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "# import tensorflow as tf\n",
    "from scipy import stats\n",
    "# Used for distributions libraries.\n",
    "from scipy import stats\n",
    "# Utility imports\n",
    "from utils.losses import *\n",
    "from utils.plotting import *\n",
    "from utils.training import *\n",
    "\n",
    "import pickle\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb772a5d-6f9f-4d95-96e5-6652cf2e4949",
   "metadata": {},
   "source": [
    "## This notebook will train BCE classifier for normalizing flow dataset using Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1f6f24-81f5-4790-a5cc-4cc27be258af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_vals = [1e-3]\n",
    "file_name = 'scan_bce_adam_results'\n",
    "loss_funcs = ['linear', 'square', 'exponl']\n",
    "\n",
    "reps = 100\n",
    "optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0718191-0647-4280-8049-c22c08ff878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "N = 10**6\n",
    "X = np.load('data/zenodo/fold/8/X_trn.npy')[:N]\n",
    "y = np.load('data/zenodo/fold/8/y_trn.npy')[:N].astype('float32')\n",
    "data, m, s = split_data(X, y)\n",
    "\n",
    "class train_val_loader(pl.LightningDataModule):\n",
    "    def __init__(self, data, N, workers):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "    def prepare_data(self):\n",
    "        X_train, X_test, y_train, y_test = self.data\n",
    "        X_train = X_train.astype(np.float32)\n",
    "        X_test = X_test.astype(np.float32)\n",
    "        y_train = y_train.astype(np.float32)\n",
    "        y_test = y_test.astype(np.float32)\n",
    "\n",
    "        self.X_train = torch.from_numpy(X_train)\n",
    "        self.X_test = torch.from_numpy(X_test)\n",
    "        self.y_train = torch.from_numpy(y_train)\n",
    "        self.y_test = torch.from_numpy(y_test)\n",
    "\n",
    "        self.train_data = TensorDataset(self.X_train, self.y_train)\n",
    "        self.test_data = TensorDataset(self.X_test, self.y_test)\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=int(0.1*self.N), shuffle=True, num_workers=self.workers, persistent_workers=True)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=int(0.1*self.N), shuffle=False, num_workers=self.workers, persistent_workers=True)\n",
    "\n",
    "train_val_data = train_val_loader(data, N, 20)\n",
    "\n",
    "max_epochs = 500\n",
    "min_epochs = 15\n",
    "patience = 10\n",
    "\n",
    "X_mae = np.load('data/zenodo/fold/8/X_tst.npy')\n",
    "X_mae = torch.from_numpy(X_mae)\n",
    "lr_tst = np.load('data/zenodo/fold/8/lr_tst.npy')\n",
    "lr_tst = torch.from_numpy(lr_tst)\n",
    "\n",
    "def mae(model_lr):\n",
    "    abs_dif = abs(model_lr(X_mae) - lr_tst)\n",
    "    return abs_dif[abs_dif < 100].mean()\n",
    "def stdae(model_lr):\n",
    "    abs_dif = abs(model_lr(X_mae) - lr_tst)\n",
    "    return abs_dif[abs_dif < 100].std()\n",
    "\n",
    "filestr = 'models/zenodo/bce/'\n",
    "dict_list = []\n",
    "for loss_func in loss_funcs:\n",
    "    for learning_rate in lr_vals:\n",
    "        for i in range(reps):\n",
    "            if loss_func == 'odds': loss_fn = bce; output = 'sigmoid'; lr_fn = odds_lr\n",
    "            elif loss_func == 'probit': loss_fn = probit_bce; output = 'linear'; lr_fn = probit_lr\n",
    "            elif loss_func == 'arctan': loss_fn = arctan_bce; output = 'linear'; lr_fn = arctan_lr\n",
    "\n",
    "            params = {'loss_fun':loss_fn, 'd': 6, 'output': output, 'optimizer': optimizer, 'learning_rate': learning_rate}\n",
    "            model_path = filestr + loss_func + '/adam/'\n",
    "\n",
    "            checkpoint_callback = ModelCheckpoint(\n",
    "                dirpath = model_path,\n",
    "                filename = 'model_{}'.format(i),\n",
    "                monitor = 'val_loss',\n",
    "                mode = 'min',\n",
    "                save_weights_only = True\n",
    "            )\n",
    "\n",
    "            trainer = pl.Trainer(accelerator='cuda', devices=1, max_epochs=max_epochs, callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience = patience), checkpoint_callback], min_epochs=min_epochs, enable_progress_bar=False)\n",
    "            \n",
    "            model = create_model_original(**params)\n",
    "            \n",
    "            trainer.fit(model, train_val_data)\n",
    "\n",
    "            try: os.mkdir(model_path)\n",
    "            except OSError as error: print(error)\n",
    "\n",
    "            train_losses = model.train_hist\n",
    "            val_losses = model.val_hist\n",
    "\n",
    "            checkpoint = torch.load(checkpoint_callback.best_model_path)\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            model.eval()\n",
    "            lr = lr_fn(model, m , s)\n",
    "            mae_1 = mae(lr).detach().numpy()\n",
    "\n",
    "            scan_res = dict(mae = mae_1, optimizer = optimizer, learning_rate = learning_rate, classifier = loss_func, train_loss = train_losses, val_loss = val_losses, path = checkpoint_callback.best_model_path, patience = patience)\n",
    "            dict_list.append(scan_res)\n",
    "            print(f'lr: {learning_rate} mae: {mae_1} classifier: ', loss_func, f'path: {model_path}')\n",
    "            del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9c7bd-a41a-4d2d-82bd-1526b7b2d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/zenodo/bce/' + file_name + '.pkl', 'wb') as fout:\n",
    "    pickle.dump(dict_list, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81857fc-8b92-4409-8f32-9effc22d8a99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
