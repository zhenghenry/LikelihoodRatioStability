{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964af9f7-5757-4337-9885-9a372e67ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility imports\n",
    "# General imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "# import tensorflow as tf\n",
    "from scipy import stats\n",
    "# Used for distributions libraries.\n",
    "from scipy import stats\n",
    "# Utility imports\n",
    "from utils.losses import *\n",
    "from utils.plotting import *\n",
    "from utils.training import *\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1f6f24-81f5-4790-a5cc-4cc27be258af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "eta = 1.6e3\n",
    "lr_vals = [1.0]\n",
    "F0 = -0.3\n",
    "nu_vals = [0]\n",
    "\n",
    "file_name = 'scan_bce_ECD_results'\n",
    "loss_funcs = ['linear', 'square', 'exponl']\n",
    "\n",
    "reps = 100\n",
    "optimizer = 'ECD'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0718191-0647-4280-8049-c22c08ff878f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "N = 10**6\n",
    "X = np.load('data/zenodo/fold/8/X_trn.npy')[:N]\n",
    "y = np.load('data/zenodo/fold/8/y_trn.npy')[:N].astype('float32')\n",
    "data, m, s = split_data(X, y)\n",
    "\n",
    "class train_val_loader(pl.LightningDataModule):\n",
    "    def __init__(self, data, N, workers):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "    def prepare_data(self):\n",
    "        X_train, X_test, y_train, y_test = self.data\n",
    "        X_train = X_train.astype(np.float32)\n",
    "        X_test = X_test.astype(np.float32)\n",
    "        y_train = y_train.astype(np.float32)\n",
    "        y_test = y_test.astype(np.float32)\n",
    "\n",
    "        self.X_train = torch.from_numpy(X_train)\n",
    "        self.X_test = torch.from_numpy(X_test)\n",
    "        self.y_train = torch.from_numpy(y_train)\n",
    "        self.y_test = torch.from_numpy(y_test)\n",
    "\n",
    "        self.train_data = TensorDataset(self.X_train, self.y_train)\n",
    "        self.test_data = TensorDataset(self.X_test, self.y_test)\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=int(0.1*self.N), shuffle=True, num_workers=self.workers, persistent_workers=True)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=int(0.1*self.N), shuffle=False, num_workers=self.workers, persistent_workers=True)\n",
    "\n",
    "train_val_data = train_val_loader(data, N, 20)\n",
    "\n",
    "max_epochs = 500\n",
    "min_epochs = 15\n",
    "patience = 10\n",
    "\n",
    "X_mae = np.load('data/zenodo/fold/8/X_tst.npy')\n",
    "X_mae = torch.from_numpy(X_mae)\n",
    "lr_tst = np.load('data/zenodo/fold/8/lr_tst.npy')\n",
    "lr_tst = torch.from_numpy(lr_tst)\n",
    "\n",
    "def mae(model_lr):\n",
    "    abs_dif = abs(model_lr(X_mae) - lr_tst)\n",
    "    return abs_dif[abs_dif < 100].mean()\n",
    "def stdae(model_lr):\n",
    "    abs_dif = abs(model_lr(X_mae) - lr_tst)\n",
    "    return abs_dif[abs_dif < 100].std()\n",
    "\n",
    "filestr = 'models/zenodo/mlc/'\n",
    "dict_list = []\n",
    "for loss_func in loss_funcs:\n",
    "    for learning_rate in lr_vals:\n",
    "        for nu in nu_vals:\n",
    "            for i in range(reps):\n",
    "                if loss_func == 'odds': loss_fn = bce; output = 'sigmoid'; lr_fn = odds_lr\n",
    "                elif loss_func == 'probit': loss_fn = probit_bce; output = 'linear'; lr_fn = probit_lr\n",
    "                elif loss_func == 'arctan': loss_fn = arctan_bce; output = 'linear'; lr_fn = arctan_lr\n",
    "        \n",
    "                params = {'loss_fun':loss_fn, 'd': 6, 'output': output, 'optimizer': optimizer, 'learning_rate': learning_rate, 'eta': eta, 'F0': F0, 'nu': nu}\n",
    "                model_path = filestr + loss_func + '/ecd/'\n",
    "    \n",
    "                checkpoint_callback = ModelCheckpoint(\n",
    "                    dirpath = model_path,\n",
    "                    filename = 'model_{}'.format(i),\n",
    "                    monitor = 'val_loss',\n",
    "                    mode = 'min',\n",
    "                    save_weights_only = True\n",
    "                )\n",
    "    \n",
    "                trainer = pl.Trainer(accelerator='cuda', devices=1, max_epochs=max_epochs, callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience = patience), checkpoint_callback], min_epochs=min_epochs, enable_progress_bar=False)\n",
    "    \n",
    "                model = create_model_original(**params)\n",
    "                model.to(device)\n",
    "                trainer.fit(model, train_val_data)\n",
    "    \n",
    "                try: os.mkdir(model_path)\n",
    "                except OSError as error: print(error)\n",
    "    \n",
    "                train_losses = model.train_hist\n",
    "                val_losses = model.val_hist\n",
    "    \n",
    "                checkpoint = torch.load(checkpoint_callback.best_model_path)\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "                model.eval()\n",
    "                lr = lr_fn(model, m , s)\n",
    "                mae_1 = mae(lr).detach().numpy()\n",
    "    \n",
    "                scan_res = dict(mae = mae_1, optimizer = optimizer, learning_rate = learning_rate, eta = eta, F0 = F0, nu = nu, classifier = loss_func, train_loss = train_losses, val_loss = val_losses, path = checkpoint_callback.best_model_path, patience=patience)\n",
    "                dict_list.append(scan_res)\n",
    "                print(f'eta: {eta} lr: {learning_rate} mae: {mae_1} classifier: ', loss_func, f'path: {model_path}')\n",
    "                del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9c7bd-a41a-4d2d-82bd-1526b7b2d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/zenodo/bce/' + file_name + '.pkl', 'wb') as fout:\n",
    "    pickle.dump(results, fout)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023a6fe-df79-4e2f-88b5-897c5fbb4f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
